{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iaD5XQMpdjs-",
        "outputId": "dc2ee43c-be6f-4220-d40c-d416f2af4768"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_24844\\1254680957.py:4: DeprecationWarning: Please import `label` from the `scipy.ndimage` namespace; the `scipy.ndimage.measurements` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.measurements import label\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'pickle' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 124\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Load the dataset from the file\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 124\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Quantize dataset into grids and get cell map\u001b[39;00m\n\u001b[0;32m    128\u001b[0m M, cell_map \u001b[38;5;241m=\u001b[39m quantize_dataset(dataset, grid_size)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
          ]
        }
      ],
      "source": [
        "import pywt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.measurements import label\n",
        "import pickle\n",
        "# from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "file_path1 = './blobs_dataset.pkl'\n",
        "file_path2 = './number.txt'\n",
        "file_path3 = './label_sets.pkl'\n",
        "\n",
        "\n",
        "def haar_wavelet_transform(matrix):\n",
        "    coeffs = pywt.dwt2(matrix, 'haar')\n",
        "    cA, (cH, cV, cD) = coeffs\n",
        "    return cA\n",
        "\n",
        "def map_cells(original_matrix):\n",
        "    mapped_cells = {}\n",
        "    for i in range(original_matrix.shape[0]):\n",
        "        for j in range(original_matrix.shape[1]):\n",
        "            # Determine the corresponding cell in the output matrix\n",
        "            output_i = i // 2\n",
        "            output_j = j // 2\n",
        "            # Update the mapping dictionary\n",
        "            output_cell = (output_i, output_j)\n",
        "            if output_cell in mapped_cells:\n",
        "                mapped_cells[output_cell].append((i, j))\n",
        "            else:\n",
        "                mapped_cells[output_cell] = [(i, j)]\n",
        "    return mapped_cells\n",
        "\n",
        "\n",
        "def get_threshold(matrix, p):\n",
        "    positive_nums = []\n",
        "    for row in matrix:\n",
        "        for element in row:\n",
        "            if element > 0:\n",
        "                positive_nums.append(element)\n",
        "    positive_nums.sort()\n",
        "\n",
        "    size = len(positive_nums)\n",
        "    k= (int)((1-p)*size)\n",
        "    return positive_nums[k-1]\n",
        "\n",
        "\n",
        "\n",
        "def identify_significant_grids(average_subband_matrix, threshold):\n",
        "    # Identify significant grids based on the threshold\n",
        "    significant_grids = average_subband_matrix > threshold\n",
        "    return significant_grids\n",
        "\n",
        "def label_clusters(significant_grids):\n",
        "    # Use connected component analysis to label clusters\n",
        "    labeled_matrix, num_clusters = label(significant_grids)\n",
        "    return labeled_matrix, num_clusters\n",
        "\n",
        "def map_clusters_to_original_cells(mapped_cells, labeled_matrix):\n",
        "    # Assign cluster numbers to the cells in the original input matrix\n",
        "    clustered_matrix = np.zeros_like(M)\n",
        "    for output_cell, original_cells in mapped_cells.items():\n",
        "        for original_cell in original_cells:\n",
        "            clustered_matrix[original_cell] = labeled_matrix[output_cell]\n",
        "    return clustered_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def quantize_dataset(dataset, grid_size):\n",
        "    \"\"\"\n",
        "    Quantize the feature space of a dataset into grids of a specified size.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset (np.ndarray): Array representing the dataset.\n",
        "    - grid_size (float): Size of each grid.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: Count matrix M representing the quantized feature space.\n",
        "    - dict: A dictionary mapping grid cell indices to the input points belonging to that cell.\n",
        "    \"\"\"\n",
        "    # Determine the number of grids in each dimension\n",
        "    x_min, y_min = np.min(dataset, axis=0)\n",
        "    x_max, y_max = np.max(dataset, axis=0)\n",
        "    num_x_grids = int(np.ceil((x_max - x_min) / grid_size))\n",
        "    num_y_grids = int(np.ceil((y_max - y_min) / grid_size))\n",
        "\n",
        "    # Initialize count matrix M\n",
        "    M = np.zeros((num_y_grids, num_x_grids), dtype=int)\n",
        "\n",
        "    # Initialize dictionary to store mapping of grid cells to input points\n",
        "    cell_map = {}\n",
        "\n",
        "    # Quantize dataset into grids\n",
        "    for index, point in enumerate(dataset):\n",
        "        x_index = int((point[0] - x_min) / grid_size)\n",
        "        y_index = int((point[1] - y_min) / grid_size)\n",
        "        cell_index = (x_index, y_index)\n",
        "        M[y_index, x_index] += 1\n",
        "        if cell_index not in cell_map:\n",
        "            cell_map[cell_index] = []\n",
        "        cell_map[cell_index].append(point)\n",
        "\n",
        "    return M, cell_map\n",
        "\n",
        "# Parameters\n",
        "num_samples = 1000\n",
        "num_clusters = 3\n",
        "cluster_std = 1.0\n",
        "grid_size = 1.0\n",
        "\n",
        "\n",
        "# Load the dataset from the file\n",
        "with open(file_path1, 'rb') as file:\n",
        "    dataset = pickle.load(file)\n",
        "\n",
        "\n",
        "# Quantize dataset into grids and get cell map\n",
        "M, cell_map = quantize_dataset(dataset, grid_size)\n",
        "\n",
        "# Plot dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(dataset[:, 0], dataset[:, 1], color='blue', label='Dataset')\n",
        "plt.title('Dataset')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Output the count matrix\n",
        "print(\"Count Matrix M:\")\n",
        "print(M)\n",
        "\n",
        "# Output the map\n",
        "# print(\"\\nMapping of Grid Cells to Input Points:\")\n",
        "# for cell_index, points in cell_map.items():\n",
        "#     print(f\"Cell {cell_index}: {points}\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "# Step 1: Apply Haar wavelet transform and retrieve average subband matrix\n",
        "average_subband_matrix = haar_wavelet_transform(M)\n",
        "\n",
        "# Output average subband matrix\n",
        "print(\"Average Subband Matrix:\")\n",
        "print(average_subband_matrix)\n",
        "\n",
        "# Step 2: Map each cell of the output matrix to the cells of the original matrix\n",
        "mapped_cells = map_cells(M)\n",
        "\n",
        "# Step 3: Define a threshold\n",
        "# threshold = 10  # Adjust the threshold as needed\n",
        "threshold=get_threshold(average_subband_matrix, 0.5)\n",
        "print(\"threshold:\")\n",
        "print(threshold)\n",
        "\n",
        "\n",
        "# Step 4: Identify significant grids\n",
        "significant_grids = identify_significant_grids(average_subband_matrix, threshold)\n",
        "\n",
        "print(\"getting significant grids:\")\n",
        "print(significant_grids)\n",
        "count_of_significant_grids = 0\n",
        "for row in significant_grids:\n",
        "        for element in row:\n",
        "            if element == True:\n",
        "                count_of_significant_grids +=1\n",
        "\n",
        "print(\"count of significant grids: \")\n",
        "print(count_of_significant_grids)\n",
        "# Write the number to a file in your Google Drive\n",
        "with open(file_path2, 'w') as file:\n",
        "    file.write(str(count_of_significant_grids))\n",
        "\n",
        "\n",
        "# Step 5: Label clusters using connected component analysis\n",
        "labeled_matrix, num_clusters = label_clusters(significant_grids)\n",
        "\n",
        "# Step 6: Map cluster numbers to original cells\n",
        "clustered_matrix = map_clusters_to_original_cells(mapped_cells, labeled_matrix)\n",
        "\n",
        "print(\"\\nClustered Matrix:\")\n",
        "print(clustered_matrix)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "# # Print cell map\n",
        "# print(\"\\nCell Map:\")\n",
        "# for cell_index, points in cell_map.items():\n",
        "#     print(f\"Cell {cell_index}: {len(points)} points\")\n",
        "\n",
        "# Generate cluster numbers matrix W\n",
        "W = clustered_matrix\n",
        "\n",
        "# Assign cluster numbers to each point in the cell map\n",
        "clustered_points = []\n",
        "for cell_index, points in cell_map.items():\n",
        "    cluster_number = W[cell_index[1], cell_index[0]]  # cell_index order is (y_index, x_index)\n",
        "    clustered_points.extend([(point, cluster_number) for point in points])\n",
        "\n",
        "# Plot points with cluster numbers\n",
        "# plt.figure(figsize=(8, 6))\n",
        "for point, cluster_number in clustered_points:\n",
        "    x, y = point\n",
        "    plt.scatter(x, y, color=plt.cm.tab10(cluster_number), label=f'Cluster {cluster_number}')\n",
        "\n",
        "plt.title('Points with Cluster Numbers')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.grid(True)\n",
        "# plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# storing data in drive\n",
        "################################################################################\n",
        "\n",
        "# Dictionary to store sets for each label\n",
        "label_sets = {}\n",
        "\n",
        "# Iterate through the points and labels\n",
        "for point, label in clustered_points:\n",
        "    if label != 0:\n",
        "        # Check if the label already exists in the dictionary\n",
        "        if label in label_sets:\n",
        "            # If label exists, add the point to its corresponding set\n",
        "            label_sets[label].add(tuple(point))\n",
        "        else:\n",
        "            # If label doesn't exist, create a new set with the point\n",
        "            label_sets[label] = {tuple(point)}\n",
        "\n",
        "# Print the sets for each label\n",
        "# for label, point_set in label_sets.items():\n",
        "#     print(f\"Points for label '{label}': {point_set}\")\n",
        "\n",
        "# Store the dictionary into a file using pickle\n",
        "with open(file_path3, 'wb') as file:\n",
        "    pickle.dump(label_sets, file)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
